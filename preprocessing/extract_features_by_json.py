import os
import glob
import math
import pandas as pd
import numpy as np
import json
import librosa
import soundfile as sf
import whisper  # pip install openai-whisper
from pydub import AudioSegment # pip install pydub
from typing import Dict, List, Any

# ==============================================================================
#  è¨­å®šã‚¨ãƒªã‚¢ (ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„)
# ==============================================================================
INPUT_JSON_DIR = "./night_run_data_parallel"    # ç·¨é›†çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
OUTPUT_FEATURE_DIR = "./input_features"         # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVå½¢å¼ï¼‰ã®å‡ºåŠ›å…ˆ
JSON_FILE_EXT = ".xml.json"
VIDEO_FILE_EXT = ".mp4"
OUTPUT_FILE_SUFFIX = "_features.csv"
TIME_STEP = 0.1                                 # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åˆ»ã¿å¹…ï¼ˆç§’ï¼‰

# Whisperãƒ¢ãƒ‡ãƒ«ã®è¨­å®š ("tiny", "base", "small", "medium", "large")
# ç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ "small" ã‚’æ¨å¥¨
WHISPER_MODEL_SIZE = "small"
# ==============================================================================

print(f"[{WHISPER_MODEL_SIZE}] ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... (ã“ã‚Œã«ã¯å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)")
model = whisper.load_model(WHISPER_MODEL_SIZE)
print("ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---

def load_xml_json(json_path: str) -> Dict[str, Any]:
    """*.xml.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)

def get_base_video_path_and_name(data: Dict[str, Any]) -> tuple[str, str, str]:
    """JSONãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€å…ƒã®å‹•ç”»ã®çµ¶å¯¾ãƒ‘ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç‰¹å®šã—ã¦è¿”ã™"""
    video_path_raw = data.get("meta", {}).get("video_path", "")
    if not video_path_raw:
        return "", "", ""
    
    # OSã®ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ã‚’æ­£è¦åŒ–
    video_path_normalized = video_path_raw.replace("\\", "/")
    
    base_name = os.path.basename(video_path_normalized)
    video_stem = base_name.replace(VIDEO_FILE_EXT, "")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«æ‹¡å¼µå­ãŒãªã„å ´åˆã®è£œå®Œ
    if "." not in base_name:
        base_name += VIDEO_FILE_EXT
        
    return video_path_normalized, base_name, video_stem

# --- ã€æœ¬ç•ªç”¨ã€‘æ–‡å­—èµ·ã“ã—å‡¦ç† ---

def get_whisper_features(audio_path: str) -> List[Dict[str, Any]]:
    """
    OpenAI Whisperã‚’ä½¿ã£ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å˜èªã”ã¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—ã™ã‚‹
    """
    try:
        # Whisperå®Ÿè¡Œ (word_timestamps=True ã§å˜èªã”ã¨ã®æ™‚é–“ã‚’å–å¾—)
        result = model.transcribe(audio_path, word_timestamps=True)
        
        word_list = []
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ -> å˜èª ã¸ã¨åˆ†è§£ã—ã¦ãƒªã‚¹ãƒˆåŒ–
        for segment in result.get('segments', []):
            for word_info in segment.get('words', []):
                word_list.append({
                    "word": word_info['word'],
                    "start": word_info['start'],
                    "end": word_info['end']
                })
        return word_list

    except Exception as e:
        print(f"  [Whisper Error] æ–‡å­—èµ·ã“ã—ã«å¤±æ•—: {e}")
        return []

def align_text_features(whisper_results: List[Dict[str, Any]], total_duration: float, time_step: float) -> pd.DataFrame:
    """
    Whisperã®çµæœï¼ˆç–ãªãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ã€0.1ç§’åˆ»ã¿ã®å¯†ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹
    """
    num_steps = int(math.ceil(total_duration / time_step))
    time_points = [round(i * time_step, 6) for i in range(num_steps + 1)]
    
    text_records = []
    
    for t in time_points:
        current_word = np.nan
        is_active = 0
        
        # ç¾åœ¨ã®æ™‚åˆ» t ãŒã€ã©ã®å˜èªã®æœŸé–“ã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for w in whisper_results:
            if w['start'] <= t < w['end']:
                current_word = w['word'].strip()
                is_active = 1
                break
        
        text_records.append({
            'time': t,
            'text_is_active': is_active,
            'text_word': current_word
        })
        
    return pd.DataFrame(text_records)

# --- ã€æœ¬ç•ªç”¨ã€‘éŸ³å£°ç‰¹å¾´é‡æŠ½å‡º & çµ±åˆé–¢æ•° ---

def extract_features_implementation(video_full_path: str, time_step: float) -> pd.DataFrame:
    """
    å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°ãƒ»ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’æŠ½å‡ºã—ã€çµ±åˆDataFrameã‚’è¿”ã™
    """
    if not os.path.exists(video_full_path):
        print(f"  [ERROR] å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_full_path}")
        return pd.DataFrame()

    temp_wav_path = "temp_process_audio.wav"
    
    try:
        print(f"  -> éŸ³å£°æŠ½å‡ºä¸­: {os.path.basename(video_full_path)} ...")
        
        # 1. PyDubã§å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºã—ã€WAV (16kHz, mono) ã«å¤‰æ›
        audio = AudioSegment.from_file(video_full_path)
        audio = audio.set_frame_rate(16000).set_channels(1)
        audio.export(temp_wav_path, format="wav")
        
        # 2. Librosaã§éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ (RMSè¨ˆç®—ç”¨)
        y, sr = librosa.load(temp_wav_path, sr=16000)
        total_duration = librosa.get_duration(y=y, sr=sr)
        
        # 3. RMS (éŸ³é‡ã‚¨ãƒãƒ«ã‚®ãƒ¼) ã®è¨ˆç®—
        # 0.1ç§’ã”ã¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ é•·ã‚’è¨­å®š
        frame_length = int(time_step * sr)
        hop_length = frame_length 
        
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        
        # 4. ç°¡æ˜“VAD (ç™ºè©±åˆ¤å®š)
        # RMSãŒã—ãã„å€¤(0.01)ã‚’è¶…ãˆãŸã‚‰ã€Œç™ºè©±ä¸­ã€ã¨ã¿ãªã™ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯
        vad_threshold = 0.01
        is_speaking = (rms > vad_threshold).astype(int)
        
        # 5. æ²ˆé»™æ™‚é–“ã®è¨ˆç®— (é€£ç¶šã™ã‚‹æ²ˆé»™ã®é•·ã•ã‚’ç´¯ç©)
        silence_duration_ms = []
        current_silence = 0
        for speak_flag in is_speaking:
            if speak_flag == 0:
                current_silence += int(time_step * 1000)
            else:
                current_silence = 0
            silence_duration_ms.append(current_silence)
        
        # 6. Whisperã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
        print(f"  -> Whisperæ–‡å­—èµ·ã“ã—å®Ÿè¡Œä¸­...")
        whisper_results = get_whisper_features(temp_wav_path)
        
        # 7. ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ™‚é–“è»¸ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        df_text = align_text_features(whisper_results, total_duration, time_step)
        
        # 8. éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameåŒ–
        # rmsã¨is_speakingã®é•·ã•ãŒæ™‚é–“è»¸ã¨å¾®å¦™ã«ãšã‚Œã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚èª¿æ•´
        min_len = min(len(rms), len(df_text))
        
        df_audio = pd.DataFrame({
            'time': df_text['time'][:min_len], # ãƒ†ã‚­ã‚¹ãƒˆå´ã®æ™‚é–“ã‚’åŸºæº–ã«ã™ã‚‹
            'audio_energy_rms': rms[:min_len],
            'audio_is_speaking': is_speaking[:min_len],
            'silence_duration_ms': silence_duration_ms[:min_len],
            # è©±è€…è­˜åˆ¥ã¯é«˜åº¦ãªãŸã‚ä»Šå›ã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ (NaN) ã¨ã—ã¾ã™
            'speaker_id': np.nan 
        })
        
        # 9. çµåˆ (Audio + Text)
        # indexã§ã¯ãªãtimeåˆ—ã§mergeã™ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ãŒã€ã“ã“ã§ã¯è¡Œæ•°ãŒæƒã£ã¦ã„ã‚‹å‰æã§concat
        df_final = pd.concat([df_audio, df_text[['text_is_active', 'text_word']].iloc[:min_len]], axis=1)
        
        return df_final

    except Exception as e:
        print(f"  [CRITICAL ERROR] è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
        if os.path.exists(temp_wav_path):
            try:
                os.remove(temp_wav_path)
            except:
                pass

# --- ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œé–¢æ•° ---

def batch_extract_all_features():
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰å‹•ç”»ã‚’ç‰¹å®šã—ã€å…¨ç‰¹å¾´é‡ã‚’ç”Ÿæˆã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    """
    os.makedirs(OUTPUT_FEATURE_DIR, exist_ok=True)
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    json_paths = glob.glob(os.path.join(INPUT_JSON_DIR, f"*{JSON_FILE_EXT}"))

    if not json_paths:
        print(f"[ERROR] JSONãƒ•ã‚©ãƒ«ãƒ€ '{INPUT_JSON_DIR}' ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"==============================================================")
    print(f"  ğŸš€ ç‰¹å¾´é‡æŠ½å‡ºãƒãƒƒãƒå‡¦ç†é–‹å§‹ ({len(json_paths)} ä»¶) ")
    print(f"==============================================================")

    for idx, json_path in enumerate(json_paths):
        print(f"--- ({idx+1}/{len(json_paths)}) JSONè§£æ: {os.path.basename(json_path)}")
        
        # JSONãƒ­ãƒ¼ãƒ‰ & ãƒ‘ã‚¹ç‰¹å®š
        data = load_xml_json(json_path)
        video_full_path, _, video_stem = get_base_video_path_and_name(data)
        
        # å‡ºåŠ›ãƒ‘ã‚¹
        output_path = os.path.join(OUTPUT_FEATURE_DIR, f"{video_stem}{OUTPUT_FILE_SUFFIX}")
        
        # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã“ã¨ã‚‚å¯èƒ½
        # if os.path.exists(output_path):
        #     print("  -> æ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        #     continue

        if not video_full_path:
            print("  [SKIPPED] å‹•ç”»ãƒ‘ã‚¹ãŒJSONã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            continue

        # â˜…â˜…â˜… ç‰¹å¾´é‡æŠ½å‡ºã®å®Ÿè¡Œ â˜…â˜…â˜…
        df_features = extract_features_implementation(video_full_path, TIME_STEP)
        
        if not df_features.empty:
            # CSVä¿å­˜
            df_features.to_csv(output_path, index=False, float_format='%.6f')
            print(f"  [SUCCESS] ä¿å­˜å®Œäº† ({len(df_features)}è¡Œ): {os.path.basename(output_path)}")
        else:
            print("  [FAILED] ç‰¹å¾´é‡ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    print("\n[å®Œäº†] å…¨ã¦ã®ãƒãƒƒãƒå‡¦ç†ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    batch_extract_all_features()