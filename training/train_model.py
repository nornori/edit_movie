import os
import glob
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import HashingVectorizer
import joblib

# ==============================================================================
#  Ë®≠ÂÆö„Ç®„É™„Ç¢
# ==============================================================================
DATASET_DIR = "./final_dataset"     # Â≠¶Áøí„Éá„Éº„Çø„Åå„ÅÇ„Çã„Éï„Ç©„É´„ÉÄ
MODEL_SAVE_PATH = "editor_ai_model.pth" # „É¢„Éá„É´„ÅÆ‰øùÂ≠òÂêç
SCALER_SAVE_PATH = "scaler.pkl"     # „Éá„Éº„ÇøÂ§âÊèõÂô®„ÅÆ‰øùÂ≠òÂêç

SEQUENCE_LENGTH = 50  # ÈÅéÂéª5ÁßíÂàÜ (0.1Áßí x 50) „ÇíË¶ã„Çã
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 0.001

# „ÉÜ„Ç≠„Çπ„ÉàÁâπÂæ¥Èáè„ÅÆÂúßÁ∏ÆÊ¨°ÂÖÉÊï∞ (Ë®ÄËëâ„Çí„ÅÑ„Åè„Å§„ÅÆÊï∞ÂÄ§„ÅßË°®Áèæ„Åô„Çã„Åã)
# 32Ê¨°ÂÖÉ„ÅÇ„Çå„Å∞„ÄÅÊó•Â∏∏‰ºöË©±„ÅÆÂçòË™û„ÅÆÈÅï„ÅÑ„Çí„ÅÇ„ÇãÁ®ãÂ∫¶Ë°®Áèæ„Åß„Åç„Åæ„Åô
TEXT_VECTOR_DIM = 32

# Ê≠£Ëß£„É©„Éô„É´Âàó
TARGET_COLS = [
    'target_is_used',       # „Ç´„ÉÉ„Éà
    'target_scale',         # „Ç∫„Éº„É†
    'target_pos_x',         # ÈÖçÁΩÆX
    'target_pos_y',         # ÈÖçÁΩÆY
    'target_graphic',       # „ÉÜ„É≠„ÉÉ„Éó
    'target_broll'          # B-roll
]
# ==============================================================================

class VideoEditorDataset(Dataset):
    def __init__(self, csv_path, scaler=None, text_vectorizer=None, sequence_length=50, is_train=True):
        self.seq_len = sequence_length
        
        # CSVË™≠„ÅøËæº„Åø
        df = pd.read_csv(csv_path)
        
        # 1. Ê≠£Ëß£„Éá„Éº„Çø (Y) „ÅÆÊäΩÂá∫
        Y_list = []
        for col in TARGET_COLS:
            if col in df.columns:
                Y_list.append(df[col].values)
            else:
                Y_list.append(np.zeros(len(df)))
        Y = np.column_stack(Y_list)
        self.Y = np.nan_to_num(Y, nan=0.0)

        # 2. ÁâπÂæ¥Èáè (X) „ÅÆÊäΩÂá∫
        # „Çø„Éº„Ç≤„ÉÉ„ÉàÂàó„Å®ÊôÇÈñìÂàó„ÇíÈô§Â§ñ
        feature_cols = [c for c in df.columns if not c.startswith('target_') and c != 'time']
        
        # --- A. Êï∞ÂÄ§„Éá„Éº„Çø„ÅÆÂá¶ÁêÜ ---
        # Êï∞ÂÄ§Âûã„ÅÆÂàó„Å†„Åë„ÇíÂèñÂæó
        numeric_df = df[feature_cols].select_dtypes(include=[np.number])
        X_numeric = numeric_df.values
        X_numeric = np.nan_to_num(X_numeric, nan=0.0)
        
        # --- B. „ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„ÅÆÂá¶ÁêÜ („Åì„Åì„Åå‰øÆÊ≠£ÁÇπÔºÅ) ---
        # 'text_word' Âàó„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Åù„Çå„Çí„Éô„ÇØ„Éà„É´Âåñ„Åô„Çã
        if 'text_word' in df.columns and text_vectorizer is not None:
            # NaN„ÇíÁ©∫ÊñáÂ≠ó„Å´Â§âÊèõ
            text_data = df['text_word'].fillna("").astype(str).tolist()
            # „Éè„ÉÉ„Ç∑„É•Âåñ (ÊñáÂ≠óÂàó -> Âõ∫ÂÆöÈï∑„ÅÆÊï∞ÂÄ§„Éô„ÇØ„Éà„É´)
            # transform „ÅØÁñéË°åÂàó„ÇíËøî„Åô„ÅÆ„Åß toarray() „Åß dense „Å´Â§âÊèõ
            X_text = text_vectorizer.transform(text_data).toarray()
        else:
            # „ÉÜ„Ç≠„Çπ„ÉàÂàó„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„Çº„É≠Âüã„ÇÅ
            X_text = np.zeros((len(df), TEXT_VECTOR_DIM))

        # --- C. Êï∞ÂÄ§„Å®„ÉÜ„Ç≠„Çπ„Éà„ÇíÁµêÂêà ---
        # Ê®™„Å´ÁµêÂêà (Êï∞ÂÄ§ÁâπÂæ¥Èáè + „ÉÜ„Ç≠„Çπ„ÉàÁâπÂæ¥Èáè)
        self.X_raw = np.hstack([X_numeric, X_text])
        
        # --- D. „Çπ„Ç±„Éº„É™„É≥„Ç∞ (Ê®ôÊ∫ñÂåñ) ---
        if is_train and scaler is not None:
            self.X = scaler.fit_transform(self.X_raw)
        elif scaler is not None:
            self.X = scaler.transform(self.X_raw)
        else:
            self.X = self.X_raw
            
        self.feature_dim = self.X.shape[1]
        self.target_dim = self.Y.shape[1]
        
    def __len__(self):
        return max(0, len(self.X) - self.seq_len)
    
    def __getitem__(self, idx):
        x_seq = self.X[idx : idx + self.seq_len]
        y_label = self.Y[idx + self.seq_len]
        return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_label, dtype=torch.float32)

# --- LSTM„É¢„Éá„É´ÂÆöÁæ© (Â§âÊõ¥„Å™„Åó) ---
class EditorAI(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, num_layers=2):
        super(EditorAI, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
        
        self.head_is_used = nn.Sequential(nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())
        self.head_scale = nn.Sequential(nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 1))
        self.head_pos = nn.Sequential(nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 2))
        self.head_triggers = nn.Sequential(nn.Linear(hidden_dim, 64), nn.ReLU(), nn.Linear(64, 2), nn.Sigmoid())

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_step = lstm_out[:, -1, :]
        return self.head_is_used(last_step), self.head_scale(last_step), self.head_pos(last_step), self.head_triggers(last_step)

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    csv_files = glob.glob(os.path.join(DATASET_DIR, "*.csv"))
    if not csv_files:
        print("[Error] „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ")
        return

    print(f"Found {len(csv_files)} datasets. preparing...")
    
    # „ÉÑ„Éº„É´ÂàùÊúüÂåñ
    scaler = StandardScaler()
    # HashingVectorizer: „Å©„Çì„Å™ÂçòË™û„Åß„ÇÇÂõ∫ÂÆöÊ¨°ÂÖÉ(32Ê¨°ÂÖÉ)„ÅÆ„Éô„ÇØ„Éà„É´„Å´Â§âÊèõ„Åô„Çã„Åô„Åî„ÅÑ„ÇÑ„Å§
    text_vectorizer = HashingVectorizer(n_features=TEXT_VECTOR_DIM, alternate_sign=False)
    
    datasets = []
    
    # ÊúÄÂàù„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅßÊ¨°ÂÖÉÊ±∫ÂÆö & fit
    try:
        first_ds = VideoEditorDataset(csv_files[0], scaler=scaler, text_vectorizer=text_vectorizer, sequence_length=SEQUENCE_LENGTH, is_train=True)
        datasets.append(first_ds)
        input_dim = first_ds.feature_dim
        print(f"Input feature dimension: {input_dim} (Numeric + Text Embedding)")
    except Exception as e:
        print(f"Error loading first dataset: {e}")
        import traceback
        traceback.print_exc()
        return

    # ÊÆã„Çä„ÅÆ„Éá„Éº„Çø„ÇíËøΩÂä†
    for f in csv_files[1:]:
        try:
            ds = VideoEditorDataset(f, scaler=scaler, text_vectorizer=text_vectorizer, sequence_length=SEQUENCE_LENGTH, is_train=False)
            if ds.feature_dim == input_dim and len(ds) > 0:
                datasets.append(ds)
        except Exception as e:
            print(f"Skipping {os.path.basename(f)}: {e}")

    if not datasets:
        print("ÊúâÂäπ„Å™„Éá„Éº„Çø„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
        return

    full_dataset = ConcatDataset(datasets)
    train_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    
    print(f"Total samples: {len(full_dataset)}")
    
    model = EditorAI(input_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # ÊêçÂ§±Èñ¢Êï∞
    criterion_cls = nn.BCELoss()
    criterion_reg = nn.MSELoss()

    print("\n--- Start Training ---")
    model.train()
    
    for epoch in range(EPOCHS):
        total_loss = 0
        for i, (x, y) in enumerate(train_loader):
            x, y = x.to(device), y.to(device)
            
            y_used = y[:, 0].unsqueeze(1)
            y_scale = y[:, 1].unsqueeze(1)
            y_pos = y[:, 2:4]
            y_trig = y[:, 4:6]
            
            p_used, p_scale, p_pos, p_trig = model(x)
            
            loss = criterion_cls(p_used, y_used) + criterion_reg(p_scale, y_scale) + criterion_reg(p_pos, y_pos) + criterion_cls(p_trig, y_trig)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if i % 100 == 0:
                print(f"Epoch {epoch+1}/{EPOCHS} | Step {i} | Loss: {loss.item():.4f}")
        
        print(f"Epoch {epoch+1} Average Loss: {total_loss / len(train_loader):.4f}")
        
        # „É¢„Éá„É´„Å®„Çπ„Ç±„Éº„É©„Éº„Çí‰øùÂ≠ò
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        joblib.dump(scaler, SCALER_SAVE_PATH)

    print(f"\nüéâ Training Finished! Model saved to {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    train()