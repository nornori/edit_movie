# 動画編集AI - 自動編集パラメータ予測システム

動画から自動的に編集パラメータ（カット位置、ズーム、クロップ、テロップ）を予測し、Premiere Pro用のXMLを生成するAIシステムです。

**想定用途**: 10分程度の動画を約2分（90秒〜150秒）のハイライト動画に自動編集

## 🎯 機能

- **マルチモーダル学習**: 音声・映像・トラックの3つのモダリティを統合
- **自動カット検出**: AIが最適なカット位置を予測
- **音声同期カット**: 映像と音声を同じ位置で自動カット
- **クリップフィルタリング**: 短すぎるクリップの除外、ギャップ結合、優先順位付け
- **Premiere Pro連携**: 生成されたXMLをそのままPremiere Proで開ける

### 将来的に実装予定の機能

- **AI字幕生成**: 音声認識（Whisper）と感情検出による自動字幕生成
- **テロップ自動配置**: OCRで検出したテロップのXML出力
- **動的な解像度対応**: 入力動画に応じた自動シーケンス設定

## 📁 プロジェクト構造

```
xmlai/
├── src/                          # ソースコード
│   ├── data_preparation/         # データ準備
│   ├── model/                    # モデル定義
│   ├── training/                 # 学習
│   ├── inference/                # 推論
│   └── utils/                    # ユーティリティ
├── scripts/                      # 補助スクリプト
├── tests/                        # テストコード
├── configs/                      # 設定ファイル
├── docs/                         # ドキュメント
├── data/                         # データ（.gitignoreで除外）
├── checkpoints/                  # 学習済みモデル（.gitignoreで除外）
├── preprocessed_data/            # 前処理済みデータ（.gitignoreで除外）
├── outputs/                      # 出力ファイル
├── archive/                      # アーカイブ（.gitignoreで除外）
└── backups/                      # バックアップ（.gitignoreで除外）
```

## 🚀 クイックスタート

### 必要な環境
- **OS**: Windows（バッチファイルを使用）
  - Mac/Linuxの場合は、Pythonコマンドを直接実行してください
- **Python**: 3.8以上
- **GPU**: CUDA対応GPU（推奨）
- **Premiere Pro**: XML読み込み用

### インストール
```bash
pip install -r requirements.txt
```

**注意**: `requirements.txt`は現在バージョン固定されていません。動作確認済みのバージョンは以下の通りです：
- torch==2.0.1
- transformers==4.30.0
- その他のライブラリは最新版で動作確認済み

### 新しい動画を自動編集

**方法1: バッチファイルを使う（推奨）**
```bash
run_inference.bat "path\to\your_video.mp4"
```

**方法2: 手動で実行**
```bash
# 推論実行
python -m src.inference.inference_pipeline "your_video.mp4" outputs/inference_results/output.xml

# Premiere Proで output.xml を開く
```

詳しくは [QUICK_START.md](QUICK_START.md) を参照してください。

## 📚 ドキュメント

### 基本ガイド
- [プロジェクト全体の流れ](docs/guides/PROJECT_WORKFLOW_GUIDE.md)
- [必要なファイル一覧](docs/guides/REQUIRED_FILES_BY_PHASE.md)
- [音声カット & テロップ変換](docs/summaries/AUDIO_CUT_AND_TELOP_GRAPHICS_SUMMARY.md)

## 🔧 開発

### データ準備
```bash
# バッチファイルで一括実行（推奨）
run_data_preparation.bat

# または手動で実行
python -m src.data_preparation.premiere_xml_parser
python -m src.data_preparation.extract_video_features_parallel
python -m src.data_preparation.data_preprocessing
```

### 学習
```bash
# バッチファイルで実行（推奨）
run_training.bat

# または手動で実行
python -m src.training.train --config configs/config_multimodal_experiment.yaml
```

### テスト
```bash
# ユニットテスト
pytest tests/unit/

# 統合テスト
pytest tests/integration/
```

## 📊 性能

- **学習データ**: 106本の編集済み動画を分割して299シーケンス（学習239 + 検証60）
- **想定入力**: 10分程度の動画
- **出力**: 約2分（90秒〜150秒）のハイライト動画
- **学習時間**: 100エポック（約2-3時間、GPU使用時）
- **推論時間**: 5~10分/動画（特徴量抽出含む）
- **カット数**: 約8〜12個のクリップ（最小3秒、ギャップ結合・優先順位付け後）
- **モデル性能**: 学習データは十分にあるが、Active閾値0.29が必要なほど確信度が低い（Loss関数の調整が必要）

## ⚠️ 既知の問題点・改善点

### 現在の問題点

#### 1. テロップ関連
- **テロップがBase64エンコードで特徴量に含められていない**
  - Premiere ProのBase64エンコード形式のため、テロップの内容や位置情報を学習に活用できていない
  - OCRで検出したテロップ情報が学習データに反映されていない
- **テロップのXML出力未対応**
  - 学習したテロップ情報をXMLに出力する機能が実装されていない
  - 現在はテロップ生成を無効化して対応（`configs/config_telop_generation.yaml`）

#### 2. クリップ生成の問題
- **✅ 解決済み: 過剰なカット問題**
  - 以前は0.1秒ごとの過剰なカットで音声・動画が飛び飛びになっていた
  - **現在の対策**:
    - 最小クリップ継続時間: 3.0秒（3秒未満のクリップは不採用）
    - ギャップ結合: クリップ間のギャップが2秒以内なら自動的に埋めて結合
    - 優先順位付け: モデルの確信度（active確率）が高いクリップを優先選択
    - 合計時間制限: 目標90秒、最大150秒に制限
  - 実装場所: `src/inference/inference_pipeline.py`

#### 3. 編集の自由度
- **単一トラック配置**
  - 現在は1つのトラックに全クリップが時系列順に配置される
  - 複数トラックへの分散配置は未実装（編集の自由度が低い）

#### 4. モデルの確信度の問題
- **Active閾値0.29が必要なほど確信度が低い**
  - 学習データは106本の動画を分割した299シーケンスと十分にある
  - しかし、Active判定に0.29という低い閾値が必要
  - **原因の可能性**:
    - 学習データの不均衡（Activeなフレームとそうでないフレームの比率）
    - Loss関数の`active_weight`調整が不十分
    - モデルがactiveクラスに対して自信を持てていない
    - 動画を分割したことによる文脈情報の損失
  - **影響**: 低い閾値により、本来不要なクリップも採用されてしまう可能性
  - 現在の性能: 約500個のカット/動画（閾値0.29の場合）

#### 5. マジックナンバーへの依存（推論時）
- **Active閾値0.29のハードコーディング**
  - `src/inference/inference_pipeline.py`内で閾値が固定されている
  ```python
  # 閾値0.29を超えるトラックの中から、最も確率が高いものを選択
  active_tracks = np.where(active_probs[t, :] > 0.29)[0]
  ```
  - **問題点**:
    - 0.29という非常に具体的かつ微妙な数値
    - モデルの学習具合によって最適値が大きく変動する
    - この低い閾値は、モデルがactiveクラスに対して確信度が低い可能性を示唆
  - **原因の可能性**:
    - 学習データの不均衡（Activeなフレームが少ない）
    - Loss関数の`active_weight`調整が不十分
  - **提案**:
    - 引数（args）やconfigファイルから設定可能にする
    - 学習データの不均衡を見直す
    - Loss関数の重み付けを調整

#### 6. フレーム単位の回帰予測のジッター
- **ScaleやPosition（x, y）の予測が不安定**
  - フレームごとに独立して予測するため、値が微妙に震える（ジッター）
  - 生成された動画で画像がガクガク震える現象が発生
  - **現在の応急処置**:
    - 採用クリップ間の短い不採用クリップを採用として扱う
    - 短い採用クリップで前後に採用クリップがない場合は不採用にする
    - クリップ全体の平均値（`np.mean`）を使用
  - **問題点**:
    - 平均化すると動き（ズームインなど）が消えてしまう
    - プロっぽい滑らかなカメラワークが実現できない
  - **提案**:
    - 移動平均フィルタ（Moving Average）の適用
    - サビツキー・ゴーレイ・フィルタなどで数値を滑らかにする
    - キーフレーム補間を考慮した予測方法の検討

#### 7. XMLパースの複雑さ
- **premiere_xml_parser.pyの制限**
  - 標準的なXML構造のみを想定
  - **対応できない構造**:
    - ネストされたシーケンス（Nested Sequence）
    - マルチカムクリップ
    - 複雑なエフェクトチェーン
  - **問題点**:
    - ネストされた構造内のクリップが無視される
    - 時間計算が正しく行えない可能性
  - **提案**:
    - 再帰的にネストを掘るロジックの実装
    - より堅牢なXMLパーサーの採用（例: OpenTimelineIO）

#### 8. シーケンス設定の未対応
- **Premiere Proのシーケンス設定が反映されない**
  - 解像度、フレームレート、アスペクト比などの設定が固定
  - 縦長動画（1080x1920）に対応しているが、他の解像度は未検証
  - **問題点**:
    - 異なる解像度の動画で正しく動作しない可能性
    - フレームレートの不一致による音ズレの可能性
  - **提案**:
    - 入力動画のメタデータから自動的にシーケンス設定を生成
    - 設定ファイルでシーケンス設定をカスタマイズ可能に

#### 9. Asset ID（素材ID）の汎用性問題
- **ファイル名ベースのAsset ID割り当ての限界**
  - `premiere_xml_parser.py`でファイル名ごとにasset_id（0-9）を割り当て
  - **問題点**:
    - 学習時と推論時で扱う動画素材（ファイル名）が異なる場合、asset_idが意味をなさない
    - 例: 学習データでasset_id=1が「笑顔の画像」だった場合、推論時にasset_id=1を予測されても、どのファイルを配置すればいいか不明
    - 新しい動画素材に対して適切なasset_idを割り当てる方法がない
  - **現在の制限**:
    - 最大10種類の素材（0-9）しか扱えない
    - 素材の意味的な関連性が考慮されていない
  - **提案**:
    - **特徴量ベースのマッチング**: ID分類ではなく、素材自体の特徴量（CLIP特徴量など）との類似度マッチングを使用
    - **役割ベースのID管理**: 「Main Camera」「Slide A」「B-Roll」のような固定された役割（Role）ベースのID管理
    - **埋め込み空間での素材選択**: 素材を埋め込み空間にマッピングし、コンテキストに応じて最適な素材を選択

### コード品質の問題点（深刻）

#### 1. `src/inference/otio_xml_generator.py`の問題（最優先で修正が必要）
- **デッドコード（到達不能コード）の存在**
  - `create_premiere_xml_with_otio`関数の冒頭で`create_premiere_xml_direct`をインポートし、即座にreturn
  - その後の「OTIOを使ってタイムラインを構築する数百行のコード」はすべて実行されない
  - **リスク**: 将来誤ってreturnを消すと、全く異なるロジックが動き出してバグになる
  - **影響**: コードを読む人を混乱させ、メンテナンス性を著しく低下させる

- **正規表現によるXML操作（アンチパターン）**
  - `_post_process_telop_to_graphics_correct`などで`re.sub`を使ってXMLタグを書き換え
  - **問題点**:
    - XMLは構造化データであり、正規表現で安全に扱うのは不可能
    - タグの属性順序や空白が変わるだけでコードが動かなくなる
    - `xml.etree.ElementTree`などのパーサーを使うべき
  - **リスク**: 非常に脆く、Premiere Proのバージョンアップで簡単に壊れる

- **ハードコードされた解像度**
  - XMLテンプレート部分に`<width>1080</width>`, `<height>1920</height>`が固定
  - **問題点**: 横長動画（1920x1080）を入力しても、強制的に縦長の設定でXMLが生成される
  - **影響**: Premiere Proで読み込んだ際にアスペクト比がおかしくなる

#### 2. `src/data_preparation/extract_video_features.py`の問題（パフォーマンス・メモリ）
- **メモリリークのリスク**
  - 動画の全フレーム（またはサンプリング毎）のデータを`records`リストに追加し続ける
  - CLIP特徴量（512次元のfloat）を含むデータをメモリに溜め込み続ける
  - **問題点**: 数十分〜数時間の長い動画を処理するとメモリ不足（OOM）でクラッシュする可能性が高い
  - **提案**: 一定数ごとにCSVに書き出してメモリを解放する（Chunk処理）実装が必要

- **CLIP特徴量の抽出頻度と補間のズレ**
  - 設定で`CLIP_STEP = 1.0`（1秒に1回）だが、全体の特徴量は`TIME_STEP = 0.1`（10FPS）で出力
  - `last_clip_emb`を保持して使い回しているため、「1秒間同じ意味内容が続く」という扱いになる
  - **問題点**: 細かいシーン変化を取り逃がす原因になる

- **例外処理の甘さ**
  - MediaPipeの初期化に失敗した場合、`self.face_mesh = None`となり、顔特徴量はすべて固定値になる
  - **問題点**: 「顔が映っていない」のか「検出器が壊れた」のか区別がつかず、学習データとしてノイズになる可能性

#### 3. `src/utils/feature_alignment.py`の問題（データ整合性）
- **CLIP特徴量のL2正規化**
  - `_align_visual_features`メソッドの最後で、CLIP特徴量をL2ノルムで割って正規化
  - **注意点**: 後段のTransformerモデルが「生の値」を期待しているのか「正規化された値」を期待しているのかによって、正しさが変わる
  - **問題点**: モデル内で`LayerNorm`等が入っているなら重複する処理になる

- **不一致な特徴量次元数のコメント**
  - コメントで「522次元」と書かれているが、実際の計算ロジックと合致しているか不明
  - `num_features`の計算ロジックが手動で調整されており（`# This is 10 features`などのコメント）、特徴量の定義が変わった際にバグが出やすい

#### 4. `src/model/loss.py`の問題（学習の安定性）
- **Lossの重み付けと除算**
  - 回帰損失（Scale, Positionなど）を計算する際、`active_count`（Activeなトラック数）で割って平均化
  - **問題点**:
    - Activeなトラックが極端に少ない（例: 1つだけ）フレームと、多い（例: 20個）フレームで、1トラックあたりのLossの重みが変わる
    - `active_count`が0に近い場合の挙動が学習を不安定にさせる可能性
  - **提案**: 「バッチ全体のActiveな総数」で割るか、Masked Meanの取り方を統一したほうが安定

### 修正の優先順位

#### 緊急（コードの整合性）
1. **`otio_xml_generator.py`の整理**
   - `create_premiere_xml_with_otio`内のデッドコードを削除
   - 正規表現ベースのXML操作をやめて、正しいXMLライブラリ操作に書き換え
   - または`direct_xml_generator.py`に処理を完全に委譲してこのファイルを削除

2. **解像度の動的取得**
   - `width`, `height`をハードコードせず、入力動画（`cv2.VideoCapture`）から取得した値を使用

#### 高優先度（パフォーマンス・安定性）
3. **特徴量抽出のメモリ対策**
   - `extract_video_features.py`で、一定フレームごとに`df.to_csv(mode='a')`するなどしてメモリを解放

4. **Loss関数の安定化**
   - `loss.py`の重み付けロジックを見直し、バッチ全体で統一的な平均化を実装

### 改善予定（優先度別）

#### 緊急（コードの整合性）
- [ ] **`otio_xml_generator.py`のリファクタリング**: デッドコードの削除、正規表現ベースのXML操作の廃止
- [ ] **解像度の動的取得**: ハードコードされた解像度を入力動画から自動取得

#### 高優先度
- [ ] **特徴量抽出のメモリ対策**: Chunk処理によるメモリ解放の実装
- [ ] **Loss関数の安定化**: 重み付けロジックの見直し
- [ ] **テロップデコード**: Base64エンコードされたテロップをデコードして特徴量に含める
- [ ] **テロップのXML出力**: 学習したテロップ情報をXMLに出力する機能を実装
- [ ] **設定ファイル化**: すべてのマジックナンバーをYAMLファイルで設定可能に
  - Active閾値（0.29）
  - クリップフィルタリングのパラメータ
  - シーケンス設定（解像度、フレームレート）
- [ ] **予測値の平滑化**: 移動平均フィルタやサビツキー・ゴーレイ・フィルタの実装
- [ ] **Asset ID管理の改善**: 特徴量ベースのマッチングまたは役割ベースのID管理
- [ ] **トラック配置改善**: 複数トラックに分散配置して編集しやすいXMLを生成

#### 中優先度
- [ ] **学習データの不均衡対策**: Loss関数の重み付け調整（最優先）
- [ ] **学習データの品質向上**: より多様な編集スタイルのデータを追加
- [ ] **XMLパーサーの強化**: ネストされたシーケンスやマルチカムクリップへの対応
- [ ] **CLIP特徴量抽出の改善**: 抽出頻度と補間のズレを解消
- [ ] **例外処理の強化**: MediaPipe初期化失敗時の適切なハンドリング
- [ ] **特徴量次元数の整合性確認**: コメントと実装の一致を確認
- [ ] **特徴量抽出の高速化**: 並列処理の最適化
- [ ] **モデルの軽量化**: 推論速度の向上
- [ ] **カット閾値の自動調整**: 動画の内容に応じて最適な閾値を自動選択

#### 低優先度
- [ ] **ユニットテストの拡充**: カバレッジ向上
- [ ] **ドキュメントの充実化**: チュートリアルやFAQの追加
- [ ] **UIの追加**: GUIベースの設定・実行ツール

### 技術的負債

#### 解決済み
- ✅ **過剰なカット問題**: ギャップ結合と最小継続時間フィルタで対応
- ✅ **音声・動画の飛び飛び問題**: クリップフィルタリングで改善

#### 未解決（機能面）
- **Base64形式の解析処理未実装**: Premiere ProのBase64エンコード形式の解析・デコード処理が必要
- **マジックナンバーのハードコーディング**: 以下の値がコード内に固定されている
  - Active閾値: 0.29（特に問題）
  - 最小クリップ継続時間: 3.0秒
  - ギャップ結合の最大長: 2.0秒
  - 目標合計時間: 90秒
  - 最大合計時間: 150秒
- **予測値のジッター**: フレーム単位の回帰予測で値が震える
- **XMLパーサーの制限**: ネストされたシーケンスやマルチカムクリップに未対応
- **シーケンス設定の固定**: 解像度やフレームレートが固定されている
- **Asset ID管理の問題**: ファイル名ベースのID割り当てで汎用性がない
- **単一トラック配置の制限**: 複数トラック対応への改修が必要
- **モデルの確信度の低さ**: Active閾値0.29が必要なほど確信度が低い（学習データは106本の動画を分割した299シーケンスと十分だが、不均衡やLoss関数の問題の可能性）

#### 未解決（コード品質）
- **デッドコードの存在**: `otio_xml_generator.py`に実行されない数百行のコードが残存
- **正規表現によるXML操作**: 脆く、メンテナンス性が低い
- **ハードコードされた解像度**: XMLテンプレートに1080x1920が固定
- **メモリリークのリスク**: 長時間動画でOOMクラッシュの可能性
- **CLIP特徴量の補間問題**: 1秒間同じ値を使い回すため、細かいシーン変化を取り逃がす
- **例外処理の不足**: MediaPipe初期化失敗時の適切なハンドリングがない
- **特徴量次元数の不一致**: コメントと実装が合致していない可能性
- **Loss関数の不安定性**: active_countによる除算で学習が不安定になる可能性

### 現在の設定値（参考）

クリップフィルタリングの設定（`src/inference/inference_pipeline.py`内）:

```python
min_clip_duration = 3.0      # 最小クリップ継続時間（秒）
max_gap_duration = 2.0       # 埋めるギャップの最大長（秒）
target_duration = 90.0       # 目標合計時間（秒）- 10分動画を約2分に圧縮
max_duration = 150.0         # 最大合計時間（秒）
active_threshold = 0.29      # Active判定の閾値
```

**想定される処理フロー**:
1. 10分（600秒）の動画を入力
2. モデルが重要なシーンを予測（Active確率）
3. ギャップ結合で短い不採用区間を埋める
4. 3秒未満のクリップを除外
5. スコア（確信度）順に並べて上位を選択
6. 合計90秒（最大150秒）のハイライト動画を生成

これらの値を調整したい場合は、現在はコードを直接編集する必要があります。
将来的には設定ファイル（YAML）で変更可能にする予定です。

## 🤝 貢献

プルリクエストを歓迎します！特に以下の分野での貢献を募集しています：
- 学習データの提供
- パフォーマンス最適化
- ドキュメントの改善
- バグ修正

## 📝 ライセンス

MIT License


