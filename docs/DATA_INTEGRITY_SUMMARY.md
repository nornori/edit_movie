# データ整合性確認レポート

## 概要

このレポートは、トレーニングデータの整合性を確認し、特徴量が正しく元動画（bandicam録画）から抽出されていることを検証した結果をまとめたものです。

## 結論

✅ **全ての特徴量は元動画（bandicam録画）から正しく抽出されています**

## 検証内容

### 1. 特徴量抽出プロセスの確認

**抽出スクリプト**: `extract_all_features.py`

```python
# XMLファイルから動画パスを抽出
video_path = extract_video_path_from_xml(xml_file)

# 抽出された動画パスから特徴量を抽出
extract_features_worker(video_path, output_dir)
```

**重要なポイント**:
- XMLファイルの`<pathurl>`タグから動画パスを読み取る
- 読み取ったパスの動画ファイルから直接特徴量を抽出
- XMLに記録されているのは**元動画（bandicam録画）のパス**

### 2. 動画ソースの確認

**検証スクリプト**: `check_video_sources.py`

**結果**:
- 総XMLファイル数: 110個
- 元動画（bandicam）: **110個** ✅
- 編集後動画: **0個** ✅
- 不明: 0個
- エラー: 0個

**例**:
```
bandicam 2025-06-11 22-48-40-994.xml
→ D:\切り抜き\2025-6\2025-6-11\bandicam 2025-06-11 22-48-40-994.mp4
```

### 3. データ整合性の確認

**検証スクリプト**: `verify_data_integrity.py`

**結果**:
- 総動画数: 106個（ラベル生成成功）
- OK: **106個** ✅
- WARNING: 0個
- ERROR: 0個

**統計**:
- タイムステップ差の平均: **0.2**
- 動画時間差の平均: **0.02秒**

これは、特徴量とラベルが完全に同期していることを示しています。

### 4. 古いラベルと新しいラベルの比較

**比較スクリプト**: `compare_old_new_labels.py`

**例1**: `bandicam 2025-06-11 22-48-40-994`
- 古いラベル: 73秒（編集後の動画の長さ）
- 新しいラベル: 235秒（元動画全体の長さ）
- **差分**: 162秒分のカット判断を学習可能に！

**例2**: `bandicam 2025-03-26 13-09-28-006`
- 古いラベル: 178秒
- 新しいラベル: 651秒
- **差分**: 473秒分のカット判断を学習可能に！

## データフロー図

```
元動画（bandicam録画）
    ↓
[特徴量抽出]
    ↓
input_features/*.csv (音声+映像特徴量)
    ↓
[XMLパース - 元動画全体]
    ↓
output_labels_full/*.csv (トラックパラメータ + カット判断)
    ↓
[トレーニングシーケンス作成]
    ↓
preprocessed_data/*.npz (299チャンク)
```

## 新しいデータ構造の利点

### 古いモデル（編集後の動画のみ）
- 学習内容: 編集後の動画内でのトラック配置のみ
- 学習できないこと: どこをカットするか

### 新しいモデル（元動画全体）
- 学習内容: **カット判断** + トラック配置
- 元動画の全タイムステップに対して:
  - `active=1`: この部分は使用（編集後の動画に含まれる）
  - `active=0`: この部分はカット（編集後の動画から削除）

## トレーニングデータの統計

### 元動画全体のラベル（output_labels_full/）
- 動画数: 106個
- 総タイムステップ: 248,147
- 使用フレーム: 118,118 (47.6%)
- カットフレーム: 130,029 (52.4%)

### トレーニングシーケンス（preprocessed_data/）
- トレーニングチャンク: 239個
- 検証チャンク: 60個
- 最大シーケンス長: 1000タイムステップ（100秒）

## 特徴量の詳細

### 音声特徴量（17次元）
- `audio_energy_rms`: RMSエネルギー
- `audio_is_speaking`: 発話検出
- `silence_duration_ms`: 無音時間
- `speaker_id`: 話者ID（未使用）
- `text_is_active`: テキスト有無
- `text_word`: 認識テキスト（文字列、学習には未使用）
- `telop_active`: テロップ有無
- `telop_text`: テロップテキスト（文字列、学習には未使用）

**学習に使用される数値特徴量**: 5次元
- audio_energy_rms, audio_is_speaking, silence_duration_ms, text_is_active, telop_active

### 映像特徴量（522次元）
- シーン転換スコア: 1次元
- 動き: 1次元
- 注目点: 2次元（x, y）
- 顔検出: 6次元（count, center_x, center_y, size, mouth_open, eyebrow_raise）
- CLIP埋め込み: 512次元

### トラックパラメータ（240次元）
- 20トラック × 12パラメータ
- パラメータ: active, asset_id, scale, pos_x, pos_y, anchor_x, anchor_y, rotation, crop_l, crop_r, crop_t, crop_b

## 検証済み事項

✅ 全ての特徴量は元動画（bandicam録画）から抽出されている
✅ 特徴量とラベルのタイムステップが完全に同期している
✅ 新しいラベルは元動画全体をカバーしている
✅ カット判断を学習できるデータ構造になっている
✅ データの整合性に問題なし

## 次のステップ

1. ✅ データ整合性確認完了
2. ⏭️ 新しいデータでモデルをトレーニング
3. ⏭️ カット判断の精度を評価
4. ⏭️ 推論パイプラインの更新

## 関連ファイル

- `extract_all_features.py` - 特徴量抽出スクリプト
- `regenerate_full_video_labels.py` - ラベル再生成スクリプト
- `verify_data_integrity.py` - データ整合性確認スクリプト
- `check_video_sources.py` - 動画ソース確認スクリプト
- `compare_old_new_labels.py` - ラベル比較スクリプト
- `data_integrity_report.json` - 詳細な検証結果
- `video_sources_check.json` - 動画ソース確認結果
